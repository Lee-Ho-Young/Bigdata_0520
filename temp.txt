onkeypress="if(event.which == 13){search(); return false;}else{return;}"



https://www.cloudera.com/documentation/enterprise/5-14-x/topics/cm_mc_yarn_service.html

-- yarn.nodemanager.resource.memory-mb :각 노드매니저 당 가용가능한 메모리 용량 
   [3개의 NodeManager, 해당값이 8로 설정되어 있으면 YARN이 가용가능한 메모리는 총 24GB]
   
-- yarn.nodemanager.resource.cpu-vcores : 컨테이너에 할당할 수 있는 가상 CPU코어의 수이다.
   [3개의 NodeManager, 해당값이 8로 설정되어 있고, 최소 할당량이 1로 되어 있으면, 총 24개의 컨테이너까지 가능하다는 말]
   
-- yarn.scheduler.minimum-allocation-vcores 

-- spark-shell 17개 키려는 순간 아래와 같은 에러 발생
ERROR ui.SparkUI: Failed to bind SparkUI
java.net.BindException: 주소가 이미 사용 중입니다: Service 'SparkUI' failed after 16 retries (starting from 4040)! Consider explicitly setting the appropriate port for the service 'SparkUI' (for example spark.ui.port for SparkUI) to an available port or increasing spark.port.maxRetries

-- spark.port.maxRetries 설정의 default = 16
-- 아래와 같이 Runtime에 설정값을 변경하여 설정 가능
spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=false

-- spark-shell --conf spark.port.maxRetries=20 명령어로 실행하니 다시 spark-shell 킬 수 있게됨

-- 하지만 총 18개의 spark-shell이 열린 이후에 다시 Pending 상태로 열리지 않음
   총 18개의 컨테이너에 대해 최대 메모리 사용량이 확보되는 상태까지만 열리는 것으로 보임

-- yarn.scheduler.maximum-allocation-vcores 4
-- yarn.scheduler.minimum-allocation-vcores 1
-- yarn.nodemanager.resource.cpu-vcores 12
-- yarn.scheduler.maximum-allocation-mb 2GB
-- yarn.scheduler. minimum-allocation-mb 1GB
-- yarn.scheduler. increment-allocation-mb 512MB
-- 


#=========================== Filebeat inputs =============================

filebeat.inputs:
- type: log
  enabled: true
  paths:
    - F:\onTuneLog\ontune_event_*.dat
  fields :
   app_group : ontune
   app_id : event

   
  ### Multiline options

  # Multiline can be used for log messages spanning multiple lines. This is common
  # for Java Stack Traces or C-Line Continuation

  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
  #multiline.pattern: ^\[

  # Defines if the pattern set under pattern should be negated or not. Default is false.
  #multiline.negate: false

  # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
  #multiline.match: after

#================================ Outputs =====================================

# Configure what output to use when sending the data collected by the beat.

#----------------------------- Logstash output --------------------------------
output.logstash:
  # The Logstash hosts
  hosts: ["150.2.237.16:8791"]  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"
#================================ Logging =====================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publish", "service".
#logging.selectors: ["*"]

​
















input 
 { beats
   {
    port=>"8790"
    codec=>plain  
   }
 } 

  output 
 { 
  if [fields][app_group] == "anycatcher"
   {
     if [fields][app_id] == "os" 
     {
      kafka
      {
        bootstrap_servers=>'150.2.237.16:6667'
        topic_id=>'1-anycatcher1-os'
        codec=>json { charset => "UTF-8" } 
      }
     }
   }
  }


$ ./bin/logstash agent –f config/logstash.config –auto-reload


pipelines.yml
- pipeline.id: sysmaster
   path.config: "/usr/share/logstash/connectconf/tibero.conf"
   pipeline.workers : 6 
- pipeline.id: anycatcher
   path.config: "/usr/share/logstash/connectconf/anycatcher.conf"
   pipeline.workers : 6
- pipeline.id: ontune
   path.config: "/usr/share/logstash/connectconf/ontune.conf"
   pipeline.workers : 4




$ nohup ./bin/logstash --path.settings /etc/logstash 1> /dev/null 2>&1 &



echo "`uname -n`||`date +'%y%m%d%H%M%S'`||CPU||`sar 1 1 | grep Average | awk '{print $3"||"$4"||"$5"||"$6"||"$7"||"$8}'`||MEM||`free -m | grep Mem | awk '{print $2"||"$3"||"$4}'`||SWAP||`free -m | grep Swap | awk '{print $2"||"$3"||"$4}'`"


